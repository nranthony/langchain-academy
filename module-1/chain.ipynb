{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cbf2458",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-1/chain.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58238466-lesson-4-chain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee55d3da-c53a-4c76-b46f-8e0d602e072e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Chain\n",
    "\n",
    "## Review\n",
    "\n",
    "We built a simple graph with nodes, normal edges, and conditional edges.\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, let's build up to a simple chain that combines 4 concepts.\n",
    "\n",
    "* Using [chat messages](https://docs.langchain.com/oss/python/langchain/messages) as our graph state\n",
    "* Using [chat models](https://docs.langchain.com/oss/python/integrations/chat) in graph nodes\n",
    "* [Binding tools](https://docs.langchain.com/oss/python/langchain/models#tool-calling) to our chat model\n",
    "* [Executing tool calls](https://docs.langchain.com/oss/python/langchain/models#tool-execution-loop) in graph nodes \n",
    "\n",
    "![Screenshot 2024-08-21 at 9.24.03 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dd607b08df5e1101_chain1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e2e80-a718-4aaf-99b9-371157b34a4b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_openai langchain_core langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ac2d0-c7b0-4a20-86e5-4b6ed15ec20e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Messages\n",
    "\n",
    "Chat models can use [messages](https://docs.langchain.com/oss/python/langchain/messages), which capture different roles within a conversation. \n",
    "\n",
    "LangChain supports various message types, including `HumanMessage`, `AIMessage`, `SystemMessage`, and `ToolMessage`. \n",
    "\n",
    "These represent a message from the user, from chat model, for the chat model to instruct behavior, and from a tool call. \n",
    "\n",
    "Let's create a list of messages. \n",
    "\n",
    "Each message can be supplied with a few things:\n",
    "\n",
    "* `content` - content of the message\n",
    "* `name` - optionally, a message author \n",
    "* `response_metadata` - optionally, a dict of metadata (e.g., often populated by model provider for `AIMessages`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "866b5321-a238-4a9e-af9e-f11a131b5f11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "So you said you were researching ocean mammals?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "Yes, that's right.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "Great, what would you like to learn about.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "I want to learn about the best place to see Orcas in the US.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "messages = [AIMessage(content=f\"So you said you were researching ocean mammals?\", name=\"Model\")]\n",
    "messages.append(HumanMessage(content=f\"Yes, that's right.\",name=\"Lance\"))\n",
    "messages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\"))\n",
    "messages.append(HumanMessage(content=f\"I want to learn about the best place to see Orcas in the US.\", name=\"Lance\"))\n",
    "\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca48df0-b639-4ff1-a777-ffe2185d991e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Chat Models\n",
    "\n",
    "Chat models use a sequence of messages as input and support message types, as discussed above.\n",
    "\n",
    "There are [many](https://docs.langchain.com/oss/python/integrations/chat) to choose from! Let's work with OpenAI. \n",
    "\n",
    "Let's check that your `OPENAI_API_KEY` is set and, if not, you will be asked to enter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2652d5ec-7602-4220-bc6e-b90783ab287b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae53d4-14f5-4bf3-a953-cc465240f5b5",
   "metadata": {},
   "source": [
    "We can load a chat model and invoke it with out list of messages.\n",
    "\n",
    "We can see that the result is an `AIMessage` with specific `response_metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b99ad4-5753-49d3-a916-a9e949722c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\", temperature=0)\n",
    "result = llm.invoke(messages)\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88d60338-c892-4d04-a83f-878de4a76a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Great question. In the United States, the most reliable and popular places to see wild orcas are:\\n\\n1) San Juan Islands / Puget Sound, Washington\\n- Why it’s great: Easy to reach from Seattle or Vancouver, with strong chances to see Southern Residents (and some Transients) during the salmon season.\\n- Best time: May–October, with peak sightings around July–September when salmon runs attract the pods.\\n- How to do it: Book a licensed whale-watching tour from Friday Harbor, Anacortes, or nearby towns. Operators follow NOAA guidelines to keep a safe distance.\\n- What you’ll see: Orcas, along with other marine life like seals and porpoises; often several pods in a day.\\n- Tips: Choose a large-boat or small-boat operator with a good track record; bring layers, expect choppy weather, and be prepared for variable seas.\\n\\n2) Southeast Alaska (Juneau, Sitka, and other Inside Passage communities)\\n- Why it’s great: Some of the most abundant and photogenic orca viewing in a stunning wilderness setting; bigger pods and a mix of residents and transients.\\n- Best time: May–September, with peak visibility in summer when conditions are calmer and wildlife is active.\\n- How to do it: Multi-hour or full-day wildlife tours depart from Juneau, Sitka, Ketchikan, etc.; cruise ships also offer onboard or shore-side whale-watching excursions.\\n- What you’ll see: Orcas (and often humpbacks, sea lions, and bald eagles) in pristine coastal scenery.\\n- Tips: This region is more weather-dependent; book with a reputable operator and be prepared for changing conditions.\\n\\nHonorable mention (less reliable than the above for pure “best sighting” but good to know):\\n- California coast (Monterey Bay and Santa Barbara Channel Islands): Orca sightings occur but are less predictable than in Washington or Alaska. If you’re in California, look for licensed tours from Monterey or Santa Barbara during peak seasons.\\n\\nA few quick planning tips\\n- Choose licensed operators with NMFS/NOAA permits who follow strict viewing guidelines to minimize impact on the whales.\\n- Know the difference: Southern Residents (Washington) rely on salmon and are more predictable in summer; Transient/Bigg’s orcas roam more widely and can appear year-round but with less consistency.\\n- Be flexible and patient. Orca watching can be weather-dependent; some days are memorable, others quieter.\\n- Ethical viewing: keep a safe distance, turn off engines when watching, don’t attempt to contact or feed the animals, and respect closures or guidelines.\\n\\nWould you like me to tailor recommendations to your location, dates, and whether you prefer a quick day trip or a longer Alaska cruise? I can suggest specific operators, sample itineraries, and estimated costs.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3275, 'prompt_tokens': 70, 'total_tokens': 3345, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 2688, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CzpPTiP11TUFfPqnjgcMfEc2VtKr1', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bd7c6-6aa7-72c1-9f00-744e2fe8b45c-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 70, 'output_tokens': 3275, 'total_tokens': 3345, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 2688}})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3a29654-6b8e-4eda-9cec-22fabb9b8620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 3275,\n",
       "  'prompt_tokens': 70,\n",
       "  'total_tokens': 3345,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 2688,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_provider': 'openai',\n",
       " 'model_name': 'gpt-5-nano-2025-08-07',\n",
       " 'system_fingerprint': None,\n",
       " 'id': 'chatcmpl-CzpPTiP11TUFfPqnjgcMfEc2VtKr1',\n",
       " 'service_tier': 'default',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d62ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.agents import create_agent\n",
    "agent = init_chat_model(\"gpt-5-nano\")\n",
    "response = agent.invoke(\"Hello World!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4718bd5c-5314-4405-a164-f1fe912ae306",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Tools\n",
    "\n",
    "Tools are useful whenever you want a model to interact with external systems.\n",
    "\n",
    "External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language. \n",
    "\n",
    "When we bind an API, for example, as a tool we given the model awareness of the required input schema.\n",
    "\n",
    "The model will choose to call a tool based upon the natural language input from the user. \n",
    "\n",
    "And, it will return an output that adheres to the tool's schema. \n",
    "\n",
    "[Many LLM providers support tool calling](https://docs.langchain.com/oss/python/integrations/chat) and [tool calling interface](https://blog.langchain.com/improving-core-tool-interfaces-and-docs-in-langchain/) in LangChain is simple. \n",
    " \n",
    "You can simply pass any Python `function` into `ChatModel.bind_tools(function)`.\n",
    "\n",
    "![Screenshot 2024-08-19 at 7.46.28 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dc1c17a7a57f9960_chain2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a942b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's showcase a simple example of tool calling!\n",
    " \n",
    "The `multiply` function is our tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928faf56-1a1a-4c5f-b97d-bd64d8e166d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f9dba",
   "metadata": {},
   "source": [
    "If we pass an input - e.g., `\"What is 2 multiplied by 3\"` - we see a tool call returned. \n",
    "\n",
    "The tool call has specific arguments that match the input schema of our function along with the name of the function to call.\n",
    "\n",
    "```\n",
    "{'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9edbe13e-cc72-4685-ac97-2ebb4ceb2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call = llm_with_tools.invoke([HumanMessage(content=f\"What is 2 multiplied by 3\", name=\"Lance\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a78178cb-fa43-45b5-be5e-5a22bda5a5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lc': 1,\n",
      " 'type': 'constructor',\n",
      " 'id': ['langchain', 'schema', 'messages', 'AIMessage'],\n",
      " 'kwargs': {'content': '6',\n",
      "            'additional_kwargs': {'refusal': None},\n",
      "            'response_metadata': {'token_usage': {'completion_tokens': 138,\n",
      "                                                  'prompt_tokens': 143,\n",
      "                                                  'total_tokens': 281,\n",
      "                                                  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
      "                                                                                'audio_tokens': 0,\n",
      "                                                                                'reasoning_tokens': 128,\n",
      "                                                                                'rejected_prediction_tokens': 0},\n",
      "                                                  'prompt_tokens_details': {'audio_tokens': 0,\n",
      "                                                                            'cached_tokens': 0}},\n",
      "                                  'model_provider': 'openai',\n",
      "                                  'model_name': 'gpt-5-nano-2025-08-07',\n",
      "                                  'system_fingerprint': None,\n",
      "                                  'id': 'chatcmpl-CzpRJdeKARSbAn3Su1wPSCjUIALEh',\n",
      "                                  'service_tier': 'default',\n",
      "                                  'finish_reason': 'stop',\n",
      "                                  'logprobs': None},\n",
      "            'type': 'ai',\n",
      "            'id': 'lc_run--019bd7c8-23f2-70d0-acbf-f7183ee6d922-0',\n",
      "            'usage_metadata': {'input_tokens': 143,\n",
      "                               'output_tokens': 138,\n",
      "                               'total_tokens': 281,\n",
      "                               'input_token_details': {'audio': 0,\n",
      "                                                       'cache_read': 0},\n",
      "                               'output_token_details': {'audio': 0,\n",
      "                                                        'reasoning': 128}},\n",
      "            'tool_calls': [],\n",
      "            'invalid_tool_calls': []}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pp\n",
    "pp(tool_call.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f970233",
   "metadata": {},
   "source": [
    "## ChatOpenAI - bind_tools - `gpt-5-nano`\n",
    "* GPT 5 nano requires explicity directive to use the tool; otherwise it calculates it itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34c9aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\", temperature=0)\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "tool_call = llm_with_tools.invoke([HumanMessage(content=f\"Use the multipy tool to calculate 2 multiplied by 3\", name=\"Lance\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c22c4836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lc': 1,\n",
      " 'type': 'constructor',\n",
      " 'id': ['langchain', 'schema', 'messages', 'AIMessage'],\n",
      " 'kwargs': {'content': '',\n",
      "            'additional_kwargs': {'refusal': None},\n",
      "            'response_metadata': {'token_usage': {'completion_tokens': 282,\n",
      "                                                  'prompt_tokens': 154,\n",
      "                                                  'total_tokens': 436,\n",
      "                                                  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
      "                                                                                'audio_tokens': 0,\n",
      "                                                                                'reasoning_tokens': 256,\n",
      "                                                                                'rejected_prediction_tokens': 0},\n",
      "                                                  'prompt_tokens_details': {'audio_tokens': 0,\n",
      "                                                                            'cached_tokens': 0}},\n",
      "                                  'model_provider': 'openai',\n",
      "                                  'model_name': 'gpt-5-nano-2025-08-07',\n",
      "                                  'system_fingerprint': None,\n",
      "                                  'id': 'chatcmpl-D05CyIsdmShIOr0XXspJgzUhVYKwe',\n",
      "                                  'service_tier': 'default',\n",
      "                                  'finish_reason': 'tool_calls',\n",
      "                                  'logprobs': None},\n",
      "            'type': 'ai',\n",
      "            'id': 'lc_run--019bdb65-0abe-7472-832e-0a7ddf23c67d-0',\n",
      "            'tool_calls': [{'name': 'multiply',\n",
      "                            'args': {'a': 2, 'b': 3},\n",
      "                            'id': 'call_sNJvpxCKjXvUcOT1bYMnHa3s',\n",
      "                            'type': 'tool_call'}],\n",
      "            'usage_metadata': {'input_tokens': 154,\n",
      "                               'output_tokens': 282,\n",
      "                               'total_tokens': 436,\n",
      "                               'input_token_details': {'audio': 0,\n",
      "                                                       'cache_read': 0},\n",
      "                               'output_token_details': {'audio': 0,\n",
      "                                                        'reasoning': 256}},\n",
      "            'invalid_tool_calls': []}}\n"
     ]
    }
   ],
   "source": [
    "pp(tool_call.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac326a",
   "metadata": {},
   "source": [
    "## ChatOpenAI - bind_tools - `gpt-4o`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bfcb722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "tool_call = llm_with_tools.invoke([HumanMessage(content=f\"What is 2 multiplied by 3\", name=\"Lance\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0ee5c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lc': 1,\n",
      " 'type': 'constructor',\n",
      " 'id': ['langchain', 'schema', 'messages', 'AIMessage'],\n",
      " 'kwargs': {'content': '',\n",
      "            'additional_kwargs': {'refusal': None},\n",
      "            'response_metadata': {'token_usage': {'completion_tokens': 17,\n",
      "                                                  'prompt_tokens': 68,\n",
      "                                                  'total_tokens': 85,\n",
      "                                                  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
      "                                                                                'audio_tokens': 0,\n",
      "                                                                                'reasoning_tokens': 0,\n",
      "                                                                                'rejected_prediction_tokens': 0},\n",
      "                                                  'prompt_tokens_details': {'audio_tokens': 0,\n",
      "                                                                            'cached_tokens': 0}},\n",
      "                                  'model_provider': 'openai',\n",
      "                                  'model_name': 'gpt-4o-2024-08-06',\n",
      "                                  'system_fingerprint': 'fp_deacdd5f6f',\n",
      "                                  'id': 'chatcmpl-CzurWgQ8HZ7w0tr0dpxLDZOfk27ik',\n",
      "                                  'service_tier': 'default',\n",
      "                                  'finish_reason': 'tool_calls',\n",
      "                                  'logprobs': None},\n",
      "            'type': 'ai',\n",
      "            'id': 'lc_run--019bd906-3360-70e0-aff3-3ab1487ae2bb-0',\n",
      "            'tool_calls': [{'name': 'multiply',\n",
      "                            'args': {'a': 2, 'b': 3},\n",
      "                            'id': 'call_d3l7WTZysGjBzrbSY1NC9A3Z',\n",
      "                            'type': 'tool_call'}],\n",
      "            'usage_metadata': {'input_tokens': 68,\n",
      "                               'output_tokens': 17,\n",
      "                               'total_tokens': 85,\n",
      "                               'input_token_details': {'audio': 0,\n",
      "                                                       'cache_read': 0},\n",
      "                               'output_token_details': {'audio': 0,\n",
      "                                                        'reasoning': 0}},\n",
      "            'invalid_tool_calls': []}}\n"
     ]
    }
   ],
   "source": [
    "pp(tool_call.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c782c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile={'max_input_tokens': 272000, 'max_output_tokens': 128000, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True} client=<openai.resources.chat.completions.completions.Completions object at 0x115c1aed0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1160b14d0> root_client=<openai.OpenAI object at 0x1145f77d0> root_async_client=<openai.AsyncOpenAI object at 0x115f14110> model_name='gpt-5-nano' model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True\n"
     ]
    }
   ],
   "source": [
    "agent = init_chat_model(\n",
    "    model=\"gpt-5-nano\",\n",
    "    )\n",
    "agent.bind_tools([multiply])\n",
    "print(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9554091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='6', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 138, 'prompt_tokens': 14, 'total_tokens': 152, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CzpMzQ5OfCFsDZuJ08L3FxiwljfR8', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bd7c4-0db7-74e1-ae4b-ace6a29e6576-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 14, 'output_tokens': 138, 'total_tokens': 152, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.invoke(\"What is 2 multiplied by 3\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c10f9a-2372-486b-9305-55b7c41ecd6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Using messages as state\n",
    "\n",
    "With these foundations in place, we can now use  [messages](https://docs.langchain.com/oss/python/langchain/overview#messages) in our graph state.\n",
    "\n",
    "Let's define our state, `MessagesState`, as a `TypedDict` with a single key: `messages`.\n",
    "\n",
    "`messages` is simply a list of messages, as we defined above (e.g., `HumanMessage`, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3699dd5c-398c-43c7-b496-fd87e55e11ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: list[AnyMessage]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211cba3e-ebba-4b91-a539-1cbc28b4a40e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Reducers\n",
    "\n",
    "Now, we have a minor problem! \n",
    "\n",
    "As we discussed, each node will return a new value for our state key `messages`.\n",
    "\n",
    "But, this new value will overwrite the prior `messages` value!\n",
    " \n",
    "As our graph runs, we want to **append** messages to our `messages` state key.\n",
    " \n",
    "We can use [reducer functions](https://docs.langchain.com/oss/python/langgraph/graph-api#reducers) to address this.\n",
    "\n",
    "Reducers specify how state updates are performed.\n",
    "\n",
    "If no reducer function is specified, then it is assumed that updates to the key should *override it* as we saw before.\n",
    " \n",
    "But, to append messages, we can use the pre-built `add_messages` reducer.\n",
    "\n",
    "This ensures that any messages are appended to the existing list of messages.\n",
    "\n",
    "We simply need to annotate our `messages` key with the `add_messages` reducer function as metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b33eb72-3197-4870-b9a3-0da8056c40c5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663e574-ba15-46be-a37c-48c8052d693b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Since having a list of messages in graph state is so common, LangGraph has a pre-built  [`MessagesState`](https://docs.langchain.com/oss/python/langgraph/graph-api#messagesstate)! \n",
    "\n",
    "`MessagesState` is defined: \n",
    "\n",
    "* With a pre-build single `messages` key\n",
    "* This is a list of `AnyMessage` objects \n",
    "* It uses the `add_messages` reducer\n",
    "\n",
    "We'll usually use `MessagesState` because it is less verbose than defining a custom `TypedDict`, as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ab516ee-eab1-4856-8210-99f1fe499672",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class MessagesState(MessagesState):\n",
    "    # Add any keys needed beyond messages, which is pre-built \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0fff7-60a2-4582-8f12-3a3ab6633d6c",
   "metadata": {},
   "source": [
    "To go a bit deeper, we can see how the `add_messages` reducer works in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23ffea76-16a5-4053-a1bc-91e0101d91dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}, name='Model', id='b3576304-72e2-4f2f-9acf-a66b0a49abe9', tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content=\"I'm looking for information on marine biology.\", additional_kwargs={}, response_metadata={}, name='Lance', id='0313237e-2090-4b2a-9ec6-8dfb65cdad3e'),\n",
       " AIMessage(content='Sure, I can help with that. What specifically are you interested in?', additional_kwargs={}, response_metadata={}, name='Model', id='75257836-6c30-4bd2-89f0-64f9e51ac410', tool_calls=[], invalid_tool_calls=[])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial state\n",
    "initial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\"),\n",
    "                    HumanMessage(content=\"I'm looking for information on marine biology.\", name=\"Lance\")\n",
    "                   ]\n",
    "\n",
    "# New message to add\n",
    "new_message = AIMessage(content=\"Sure, I can help with that. What specifically are you interested in?\", name=\"Model\")\n",
    "\n",
    "# Test\n",
    "add_messages(initial_messages , new_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485adccc-f262-49dd-af4f-a30e9b6a48e2",
   "metadata": {},
   "source": [
    "## Our graph\n",
    "\n",
    "Now, lets use `MessagesState` with a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5306639-7e6a-44be-8471-8d2631701cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "    \n",
    "# Node\n",
    "def tool_calling_llm(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_edge(\"tool_calling_llm\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8909771-7786-47d6-a53d-6bbc3b365737",
   "metadata": {},
   "source": [
    "If we pass in `Hello!`, the LLM responds without any tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "983e2487-c0a5-40a2-afbc-aa53ff49fefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi there! Nice to meet you. How can I help today? I can answer questions, explain concepts, help with writing or editing, assist with coding or math, plan a project, or brainstorm ideas. Tell me what you’d like to do or what topic you’re interested in.\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\": HumanMessage(content=\"Hello!\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3588688b-efd9-4dbc-abf2-7903e3ef89ba",
   "metadata": {},
   "source": [
    "The LLM chooses to use a tool when it determines that the input or task requires the functionality provided by that tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7fe8b042-ecc8-426f-995e-cc1bbaf7cacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_Hhv0avpFKzQRMf39eUEZHfWz)\n",
      " Call ID: call_Hhv0avpFKzQRMf39eUEZHfWz\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\": HumanMessage(content=\"Multiply 2 and 3\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311fbae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
